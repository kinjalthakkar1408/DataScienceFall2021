{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_car_models_classifier_(1) (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wkft3nsUf5w"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "root_ = '/content'\n",
        "drive_dir = os.path.join(root_,'drive','MyDrive')\n",
        "train_ = '/dataset/train'\n",
        "os.chdir(os.path.join(root_))"
      ],
      "metadata": {
        "id": "wZkKyIR-r9B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/dog-breed-identification.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset/\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "y07I2I1dr_l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/dataset/valid')"
      ],
      "metadata": {
        "id": "d4bnSBqGsC7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "labels_csv = pd.read_csv('/content/dataset/labels.csv')"
      ],
      "metadata": {
        "id": "RXctkHt6sGdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/dataset/train/')\n",
        "  \n",
        "for c in random.sample(glob.glob('*.jpg'),2000):\n",
        "  shutil.move(c,os.path.join('/content/dataset/valid/'))"
      ],
      "metadata": {
        "id": "PxSyRbF9sJe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for breed in labels_csv.breed.unique():\n",
        "  os.makedirs(os.path.join('/content/dataset/train',breed))\n",
        "  os.makedirs(os.path.join('/content/dataset/valid',breed))"
      ],
      "metadata": {
        "id": "GDCaK7h_sLjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/dataset/train'#os.path.join(root_,'train')\n",
        "test_path = '/content/dataset/test'#os.path.join(root_,'test')\n",
        "#valid_path = '/content/dataset/valid'#os.path.join(root_,'valid')"
      ],
      "metadata": {
        "id": "6C7RGJxZsQ4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hjNwldc8sTDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/dataset/train/')\n",
        "\n",
        "for id,breed in zip(labels_csv.id,labels_csv.breed):\n",
        "  \n",
        "  try:\n",
        "    shutil.move('/content/dataset/train/'+id+'.jpg','/content/dataset/train/'+breed)\n",
        "  except:\n",
        "    shutil.move('/content/dataset/valid/'+id+'.jpg','/content/dataset/valid/'+breed)"
      ],
      "metadata": {
        "id": "LK5vCESZsNT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYdee71UJDSF"
      },
      "source": [
        "# **Steps** <p>\n",
        "Step 1: Load Dataset <p>\n",
        "Step 2: Transform the Dataset <p>\n",
        "Step 3: Create Model <p>\n",
        "Step 4: Train Model <p>\n",
        "Step 5: Save the Model <p>\n",
        "Step 6: Load the Model <p>\n",
        "Step 7: Predict the Image <p>\n",
        "Step 8: Show the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T7noSJhJUFr"
      },
      "source": [
        "# **Step 1: Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB_62Or7PxYY"
      },
      "source": [
        "#cp -av /content/drive/MyDrive/Dataset/ImageClassification-20211203T192447Z-001.zip ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip  \"ImageClassification-20211203T192447Z-001.zip\""
      ],
      "metadata": {
        "id": "wFKLOmzFYj-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GYD6bEHYhnA"
      },
      "source": [
        "#cp -av /content/drive/MyDrive/Dataset/Dataset.zip ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeRV5CjnJvN9"
      },
      "source": [
        "#!unzip  \"Dataset.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA6TES-NPHB3"
      },
      "source": [
        "# **Step 2: Importing Important Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwyPMwtZ6Z05"
      },
      "source": [
        "#Import lib\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.optimizers import gradient_descent_v2\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import TensorBoard as tensorboard\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "# from pyimagesearch.smallervggnet import SmallerVGGNet\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqu8IW5S7l-1"
      },
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# batch size, and image dimensions\n",
        "EPOCHS = 10\n",
        "INIT_LR = 1e-3\n",
        "BS = 5\n",
        "IMAGE_DIMS = (96, 96, 3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p1bF4wt7l7z"
      },
      "source": [
        "imagePaths = sorted(list(paths.list_images(\"/content/dataset/train/\")))\n",
        "random.seed(42)\n",
        "random.shuffle(imagePaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDvSRccq7l5E"
      },
      "source": [
        "data = []\n",
        "labels = []\n",
        "\n",
        "# loop over the input images\n",
        "for imagePath in imagePaths:\n",
        "\t# load the image, pre-process it, and store it in the data list\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
        "\timage = img_to_array(image)\n",
        "\tdata.append(image)\n",
        "\n",
        "\t# extract set of class labels from the image path and update the\n",
        "\t# labels list\n",
        "\tl = label = imagePath.split(os.path.sep)[-2].split(\"_\")\n",
        "\tlabels.append(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del image"
      ],
      "metadata": {
        "id": "SoIuBlxx6NVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#labels = ['affenpinscher','afghan_hound','african_hunting_dog','airedale','american_staffordshire_terrier','appenzeller','australian_terrier','basenji','basset','beagle','bedlington_terrier','bernese_mountain_dog','black-and-tan_coonhound','blenheim_spaniel','bloodhound','bluetick','border_collie','border_terrier','borzoi','boston_bull','bouvier_des_flandres','boxer','brabancon_griffon','briard','brittany_spaniel','bull_mastiff','cairn','cardigan','chesapeake_bay_retriever','chihuahua','chow','clumber','cocker_spaniel','collie','curly-coated_retriever','dandie_dinmont','dhole','dingo','doberman','english_foxhound','english_setter','english_springer','entlebucher','eskimo_dog','flat-coated_retriever','french_bulldog','german_shepherd','german_short-haired_pointer','giant_schnauzer','golden_retriever','gordon_setter','great_dane','great_pyrenees','greater_swiss_mountain_dog','groenendael','ibizan_hound','irish_setter','irish_terrier','irish_water_spaniel','irish_wolfhound','italian_greyhound','japanese_spaniel','keeshond','kelpie','kerry_blue_terrier','komondor','kuvasz','labrador_retriever','lakeland_terrier','leonberg','lhasa','malamute','malinois','maltese_dog','mexican_hairless','miniature_pinscher','miniature_poodle','miniature_schnauzer','newfoundland','norfolk_terrier','norwegian_elkhound','norwich_terrier','old_english_sheepdog','otterhound','papillon','pekinese','pembroke','omeranian','pug','redbone','rhodesian_ridgeback','rottweiler','saint_bernard','saluki','samoyed','schipperke','scotch_terrier','scottish_deerhound','sealyham_terrier','shetland_sheepdog','shih-tzu','siberian_husky','silky_terrier','soft-coated_wheaten_terrier','staffordshire_bullterrier','standard_poodle','standard_schnauzer','sussex_spaniel','tibetan_mastiff','tibetan_terrier','toy_poodle','toy_terrier','vizsla','walker_hound','weimaraner','welsh_springer_spaniel','west_highland_white_terrier','whippet','wire-haired_fox_terrier','yorkshire_terrier']"
      ],
      "metadata": {
        "id": "mlh--ptiu0Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6cQYeZHCQOg"
      },
      "source": [
        "### To check the total number of images present in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZR3UKZ7l2Z"
      },
      "source": [
        "# scale the raw pixel intensities to the range [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "print(\"[INFO] data matrix: {} images ({:.2f}MB)\".format(\n",
        "\tlen(imagePaths), data.nbytes / (1024 * 1000.0)))\n",
        "\n",
        "# binarize the labels using scikit-learn's special multi-label\n",
        "# binarizer implementation\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(labels)\n",
        "print(\"[INFO] class labels:\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac37eg5PaoFc"
      },
      "source": [
        "# **Checking the Labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXkkcXfV8Bqc"
      },
      "source": [
        "# loop over each of the possible class labels and show them\n",
        "for (i, label) in enumerate(mlb.classes_):\n",
        "\tprint(\"{}. {}\".format(i + 1, label))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvwnHgvra1p_"
      },
      "source": [
        "# **Train-Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2nBiY_48Bnh"
      },
      "source": [
        "# the data for training and the remaining 20% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data,\n",
        "\tlabels, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del data"
      ],
      "metadata": {
        "id": "y6rXI6Wm9VnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del labels"
      ],
      "metadata": {
        "id": "giqyEtG-9XwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BZbzYGQbMEC"
      },
      "source": [
        "# **Data Augmentation**\n",
        "\n",
        "### Generated a data augmentation inorder to increase the amount of dataset and avoid overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6INpLGAI8BlI"
      },
      "source": [
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rescale=1./255,rotation_range=25, width_shift_range=0.1,\n",
        "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml1GjZxybkXv"
      },
      "source": [
        "# **Creating a OneLayerNetwork Model**\n",
        "\n",
        "### Added activation function as SELU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-5HxtN_M9bL"
      },
      "source": [
        "  def onelayernetwork(width, height, depth, classes, finalAct):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t# CONV => RELU => POOL\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n",
        "\t\t\tinput_shape=inputShape))\n",
        "\t\tmodel.add(Activation(\"selu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\t\tmodel.add(Dropout(0.3))\n",
        "\n",
        "\t\t# # (CONV => RELU) * 2 => POOL\n",
        "\t\t#model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "\t\t#model.add(Activation(\"selu\"))\n",
        "\t\t#model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t#model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "\t\t#model.add(Activation(\"selu\"))\n",
        "\t\t#model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t#model.add(Dropout(0.25))\n",
        "\n",
        "\t\t# # (CONV => RELU) * 2 => POOL\n",
        "\t\t# model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "\t\t# model.add(Activation(\"selu\"))\n",
        "\t\t# model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t# model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "\t\t# model.add(Activation(\"selu\"))\n",
        "\t\t# model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t# model.add(Dropout(0.25))\n",
        "\n",
        "\t\t# first (and only) set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(1024))\n",
        "\t\tmodel.add(Activation(\"selu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes))\n",
        "\t\tmodel.add(Activation(finalAct))\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqCm3VZA8Bim"
      },
      "source": [
        "model1 = onelayernetwork(\n",
        "\twidth=IMAGE_DIMS[1], height=IMAGE_DIMS[0],\n",
        "\tdepth=IMAGE_DIMS[2], classes=len(mlb.classes_),\n",
        "\tfinalAct=\"sigmoid\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrEmmrSNcLV0"
      },
      "source": [
        "# **Gradient Estimation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN7O6e5z8Bf9"
      },
      "source": [
        "# initialize the optimizer (SGD is sufficient)\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard"
      ],
      "metadata": {
        "id": "tgd8bHQLJwZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "57z_-Ar0Lha9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
      ],
      "metadata": {
        "id": "heswH3_VJ2V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lnuy4occfNq"
      },
      "source": [
        "# **Cost Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsT-Kuym7lzz"
      },
      "source": [
        "# compile the model using binary cross-entropy rather than\n",
        "# categorical cross-entropy -- this may seem counterintuitive for\n",
        "# multi-label classification, but keep in mind that the goal here\n",
        "# is to treat each output label as an independent Bernoulli\n",
        "# distribution\n",
        "model1.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorboard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "CZlthxB3KBxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqGLDekF8Twv"
      },
      "source": [
        "H = model1.fit_generator(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tepochs=EPOCHS, verbose=1,callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model \n"
      ],
      "metadata": {
        "id": "pDcsu9P0R1dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "-s35qG3gP5wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "_d-OMySqMSY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboard import notebook\n",
        "notebook.list()"
      ],
      "metadata": {
        "id": "W_36z5zdRD8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ltJ8jvV8TuT"
      },
      "source": [
        "print(\"[INFO] serializing network...\")\n",
        "model1.save(\"Car1.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sfLe6Sd8Trj"
      },
      "source": [
        "model1.save_weights(\"model1.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb-fJgB78To6"
      },
      "source": [
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "N = EPOCHS\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "# plt.savefig(args[\"plot\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R_gPf1Z8TmL"
      },
      "source": [
        "f = open(\"mlb.pickle\", \"wb\")\n",
        "f.write(pickle.dumps(mlb))\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj7VlQhPAZ5i"
      },
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfP-cWxLCz4k"
      },
      "source": [
        "pip install imutils \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzD3bBFLC_fw"
      },
      "source": [
        "import imutils "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvEdDe7zAGxi"
      },
      "source": [
        "image = cv2.imread(\"/content/Dataset/Damaged Images/113198_PhotoGrid_1474238007772.jpg\")\n",
        "output = imutils.resize(image, width=600)\n",
        " \n",
        "# pre-process the image for classification\n",
        "image = cv2.resize(image, (96, 96))\n",
        "image = image.astype(\"float\") / 255.0\n",
        "image = img_to_array(image)\n",
        "image = np.expand_dims(image, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VypB8wVAGuz"
      },
      "source": [
        "mlb = pickle.loads(open(\"mlb.pickle\", \"rb\").read())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xez00FWuAGsC"
      },
      "source": [
        "print(\"[INFO] classifying image...\")\n",
        "proba = model1.predict(image)[0]\n",
        "idxs = np.argsort(proba)[::-1][:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1mDuyQQDMAr"
      },
      "source": [
        "proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fODcyBi1DiGs"
      },
      "source": [
        "label2={}\n",
        "\n",
        "# loop over the indexes of the high confidence class labels\n",
        "for (i, j) in enumerate(idxs):\n",
        "\t# build the label and draw the label on the image\n",
        "    #if (mlb.classes_[\"Front-smash\"])\n",
        "\t#label = \"{}: {:.2f}%\".format(mlb.classes_[j], proba[j] * 100)\n",
        "    \n",
        "\tlabel2.update({mlb.classes_[j]:proba[j] * 100})\n",
        "\n",
        "print(type(mlb.classes_))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igtFxPOkDlBH"
      },
      "source": [
        "label2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a TwoLayerNetwork Model**\n",
        "\n",
        "### Added activation function as tanh"
      ],
      "metadata": {
        "id": "XLqug8K_oQCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# batch size, and image dimensions\n",
        "EPOCHS = 10\n",
        "INIT_LR = 1e-3\n",
        "BS = 32\n",
        "IMAGE_DIMS = (96, 96, 3)"
      ],
      "metadata": {
        "id": "fJEn6jV33ygR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def twolayernetwork(width, height, depth, classes, finalAct):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t# CONV => RELU => POOL\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n",
        "\t\t\tinput_shape=inputShape))\n",
        "\t\tmodel.add(Activation(\"tanh\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# (CONV => RELU) * 2 => POOL\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"tanh\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"tanh\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# (CONV => RELU) * 2 => POOL\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"selu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"selu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# first (and only) set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(1024))\n",
        "\t\tmodel.add(Activation(\"tanh\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# sigmoid classifier\n",
        "\t\tmodel.add(Dense(classes))\n",
        "\t\tmodel.add(Activation(finalAct))\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "metadata": {
        "id": "C0K115LaooUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = twolayernetwork(\n",
        "\twidth=IMAGE_DIMS[1], height=IMAGE_DIMS[0],\n",
        "\tdepth=IMAGE_DIMS[2], classes=len(mlb.classes_),\n",
        "\tfinalAct=\"sigmoid\")"
      ],
      "metadata": {
        "id": "OZPkmu1Tovcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the optimizer (SGD is sufficient)\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)"
      ],
      "metadata": {
        "id": "QZ-zbeSPo09a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard"
      ],
      "metadata": {
        "id": "k2BjzqlTpP-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "go0bZzgapP-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
      ],
      "metadata": {
        "id": "yKFNxcxXpP-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAxngjL3pake"
      },
      "source": [
        "# **Cost Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECKm7xD6pake"
      },
      "source": [
        "# compile the model using binary cross-entropy rather than\n",
        "# categorical cross-entropy -- this may seem counterintuitive for\n",
        "# multi-label classification, but keep in mind that the goal here\n",
        "# is to treat each output label as an independent Bernoulli\n",
        "# distribution\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorboard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "ajiZgEazpakf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LALGrWPpakf"
      },
      "source": [
        "H = model.fit_generator(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tepochs=EPOCHS, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}